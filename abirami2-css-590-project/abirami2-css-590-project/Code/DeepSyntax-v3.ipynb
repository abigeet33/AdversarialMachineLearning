{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pandas import pivot_table\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer,sent_tokenize,word_tokenize\n",
    "from nltk import RegexpParser\n",
    "import re\n",
    "\n",
    "train = pd.read_csv(\"data/FakeNews_Training_Set.csv\", header=0, delimiter=\",\")\n",
    "test = pd.read_csv(\"data/FakeNews_Test_Set.csv\", header=0, delimiter=\",\")\n",
    "label_mapping = {\n",
    "           'REAL': 1,\n",
    "           'FAKE': 0}\n",
    "\n",
    "train['label'] = train['label'].map(label_mapping)\n",
    "test['label'] = test['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regexAllComboTags=['NP: {<CD><NN>}',\n",
    "'NP: {<CD><CD>}',\n",
    "'NP: {<CD><NN>}',\n",
    "'NP: {<CD><NNPS>}',\n",
    "'NP: {<CD><NNS>}',\n",
    "'NP: {<CD><NNS>}',\n",
    "'NP: {<DT><CD>}',\n",
    "'NP: {<DT><JJ>}',\n",
    "'NP: {<DT><JJS>}',\n",
    "'NP: {<DT><NN>}',\n",
    "'NP: {<DT><NNP>}',\n",
    "'NP: {<DT><NNPS>}',\n",
    "'NP: {<DT><NNS>}',\n",
    "'NP: {<DT><RB>}',\n",
    "'NP: {<DT><VBG>}',\n",
    "'NP: {<DT><VBN>}',\n",
    "'NP: {<FW><FW>}',\n",
    "'NP: {<FW><NNP>}',\n",
    "'NP: {<IN><CD>}',\n",
    "'NP: {<IN><NN>}',\n",
    "'NP: {<JJ><NN>}',\n",
    "'NP: {<JJ><NNP>}',\n",
    "'NP: {<JJ><NNPS>}',\n",
    "'NP: {<JJ><NNS>}',\n",
    "'NP: {<JJR><JJ>}',\n",
    "'NP: {<JJR><NNS>}',\n",
    "'NP: {<JJS><NNS>}',\n",
    "'NP: {<NN>}',\n",
    "'NP: {<NN><JJ>}',\n",
    "'NP: {<NN><NN>}',\n",
    "'NP: {<NN><NNP>}',\n",
    "'NP: {<NN><NNS>}',\n",
    "'NP: {<NN><POS>}',\n",
    "'NP: {<NN><RB>}',\n",
    "'NP: {<NNP>}',\n",
    "'NP: {<NNP><CD>}',\n",
    "'NP: {<NNP><NN>}',\n",
    "'NP: {<NNP><NNP>}',\n",
    "'NP: {<NNP><NNPS>}',\n",
    "'NP: {<NNP><NNS>}',\n",
    "'NP: {<NNP><POS>}',\n",
    "'NP: {<NNPS><POS>}',\n",
    "'NP: {<NNS>}',\n",
    "'NP: {<NNS><NNS>}',\n",
    "'NP: {<NNS><POS>}',\n",
    "'NP: {<PDT><DT>}',\n",
    "'NP: {<PRP><NN>}',\n",
    "'NP: {<RB>}',\n",
    "'NP: {<RB><CD>}',\n",
    "'NP: {<RB><DT>}',\n",
    "'NP: {<RB><JJ>}',\n",
    "'NP: {<RB><NN>}',\n",
    "'NP: {<RB><NNS>}',\n",
    "'NP: {<UH>}',\n",
    "'NP: {<VBG><NN>}',\n",
    "'NP: {<VBG><NNS>}',\n",
    "'NP: {<VBN><NN>}',\n",
    "'NP: {<VBN><NNS>}',\n",
    "'NP: {<WDT><NN>}',\n",
    "'PP: {<IN><JJ>}',\n",
    "'PP: {<IN><RB>}',\n",
    "'RB: {<FW><FW>}',\n",
    "'RB: {<IN>}',\n",
    "'RB: {<IN><JJS>}',\n",
    "'RB: {<JJ>}',\n",
    "'RB: {<RB>}',\n",
    "'RB: {<RB><IN>}',\n",
    "'RB: {<RB><RB>}',\n",
    "'RB: {<RB><RBR>}',\n",
    "'VP: {<CD><NN>}',\n",
    "'VP: {<CD><NNS>}',\n",
    "'VP: {<JJ><JJ><JJ>}',\n",
    "'VP: {<JJ><JJ><NN>}',\n",
    "'VP: {<JJ><JJ><RB>}',\n",
    "'VP: {<JJ><NN>}',\n",
    "'VP: {<JJ><NN><JJ>}',\n",
    "'VP: {<JJ><NN><VBN>}',\n",
    "'VP: {<JJ><RB><JJ>}',\n",
    "'VP: {<JJ><RB><VBN>}',\n",
    "'VP: {<JJ><VBG><JJ>}',\n",
    "'VP: {<VB><JJ>}',\n",
    "'VP: {<VB><RB>}',\n",
    "'VP: {<VBD><IN>}',\n",
    "'VP: {<VBD><JJ>}',\n",
    "'VP: {<VBD><RB>}',\n",
    "'VP: {<VBG><JJ>}',\n",
    "'VP: {<VBG><RB>}',\n",
    "'VP: {<VBN><IN>}',\n",
    "'VP: {<VBN><JJ>}',\n",
    "'VP: {<VBP><JJ>}',\n",
    "'VP: {<VBZ><IN>}',\n",
    "'VP: {<VBZ><JJ>}',\n",
    "'VP: {<VBZ><RB>}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a list containing 5 lists, each of 8 items, all set to 0\n",
    "\n",
    "def processGrammer(dfTrain):\n",
    "\n",
    "    matrix = np.zeros(shape=(len(dfTrain.index),94))\n",
    "\n",
    "    #print (matrix.shape)\n",
    "\n",
    "    for index,row in dfTrain.iterrows():\n",
    "\n",
    "        if index % 50 == 0:\n",
    "            print (index)\n",
    "        twlist=[]\n",
    "        sentListTrain =  sent_tokenize(row['text'])\n",
    "        matrix[index,0]=row['sno']\n",
    "\n",
    "        for sent in sentListTrain:\n",
    "            #print (sent)\n",
    "            #recvListTrain = process_content(sent)\n",
    "            words= word_tokenize(sent)\n",
    "            recvListTrain=nltk.pos_tag(words)\n",
    "\n",
    "            for tagindex,multiPOSCombo in enumerate(regexAllComboTags):\n",
    "                cp = nltk.RegexpParser(multiPOSCombo)\n",
    "                result = cp.parse(recvListTrain)\n",
    "                \n",
    "                for subtree in result:\n",
    "                    if type(subtree) == nltk.tree.Tree:\n",
    "                        matrix[index,tagindex + 1]=matrix[index,tagindex + 1] + 1\n",
    "                        #print (tagindex + 1,index)\n",
    "\n",
    "\n",
    "    matrix.astype(int)\n",
    "    #np.savetxt(\"numpy.out\", matrix, newline=\" \")\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.rename(columns={0: 'sno'}, inplace=True)\n",
    "    #print (df.columns)\n",
    "    df_merged = df.merge(dfTrain[['sno','label']],how=\"inner\",on='sno')\n",
    "    \n",
    "    print (df_merged.columns)\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "\n",
    "\n",
    "print ('Training data to Extract and Pivot')\n",
    "\n",
    "postags_merged = processGrammer(train)\n",
    "#postags_merged = postags_merged.drop(['sno', 'sno_1'], axis=1)\n",
    "\n",
    "print ('Training model')\n",
    "\n",
    "# print ( postags_merged )\n",
    "\n",
    "forest = forest.fit( postags_merged.drop(['label','sno'], axis=1), postags_merged[\"label\"] )\n",
    "\n",
    "print ('Tesing data to Extract and Pivot')\n",
    "\n",
    "test_postags_merged = processGrammer(test)\n",
    "#test_postags_merged = test_postags_merged.drop(['sno', 'sno_1'], axis=1)\n",
    "\n",
    "print ('Model Prediction started')\n",
    "\n",
    "print ( test_postags_merged )\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_postags_merged.drop(['label', 'sno'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "confusion_matrix(test_postags_merged['label'], result)\n",
    "accuracy_score(test_postags_merged['label'], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_postags_merged.to_csv(\"test_postag1.csv\", sep='\\t')\n",
    "postags_merged.to_csv(\"train_postag1.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data to Extract and Pivot\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "Index([  'sno',       1,       2,       3,       4,       5,       6,       7,\n",
      "             8,       9,      10,      11,      12,      13,      14,      15,\n",
      "            16,      17,      18,      19,      20,      21,      22,      23,\n",
      "            24,      25,      26,      27,      28,      29,      30,      31,\n",
      "            32,      33,      34,      35,      36,      37,      38,      39,\n",
      "            40,      41,      42,      43,      44,      45,      46,      47,\n",
      "            48,      49,      50,      51,      52,      53,      54,      55,\n",
      "            56,      57,      58,      59,      60,      61,      62,      63,\n",
      "            64,      65,      66,      67,      68,      69,      70,      71,\n",
      "            72,      73,      74,      75,      76,      77,      78,      79,\n",
      "            80,      81,      82,      83,      84,      85,      86,      87,\n",
      "            88,      89,      90,      91,      92,      93, 'label'],\n",
      "      dtype='object')\n",
      "Tesing data to Extract and Pivot\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "Index([  'sno',       1,       2,       3,       4,       5,       6,       7,\n",
      "             8,       9,      10,      11,      12,      13,      14,      15,\n",
      "            16,      17,      18,      19,      20,      21,      22,      23,\n",
      "            24,      25,      26,      27,      28,      29,      30,      31,\n",
      "            32,      33,      34,      35,      36,      37,      38,      39,\n",
      "            40,      41,      42,      43,      44,      45,      46,      47,\n",
      "            48,      49,      50,      51,      52,      53,      54,      55,\n",
      "            56,      57,      58,      59,      60,      61,      62,      63,\n",
      "            64,      65,      66,      67,      68,      69,      70,      71,\n",
      "            72,      73,      74,      75,      76,      77,      78,      79,\n",
      "            80,      81,      82,      83,      84,      85,      86,      87,\n",
      "            88,      89,      90,      91,      92,      93, 'label'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "labels ['label' 'sno'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4a1510d3dbb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;31m# check the accuracy on the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_postags_merged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sno'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostags_merged\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[1;31m# predict class labels for the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_postags_merged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sno'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abira\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   1905\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abira\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3260\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3261\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3262\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3263\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['label' 'sno'] not contained in axis"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# instantiate a logistic regression model, and fit with X and y\n",
    "lr = LogisticRegression()\n",
    "\n",
    "print ('Training data to Extract and Pivot')\n",
    "postags_merged = processGrammer(train)\n",
    "\n",
    "lr = lr.fit(postags_merged.drop(['label','sno'], axis=1), postags_merged[\"label\"])\n",
    "\n",
    "print ('Tesing data to Extract and Pivot')\n",
    "test_postags_merged = processGrammer(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([  'sno',       1,       2,       3,       4,       5,       6,       7,\n",
      "             8,       9,      10,      11,      12,      13,      14,      15,\n",
      "            16,      17,      18,      19,      20,      21,      22,      23,\n",
      "            24,      25,      26,      27,      28,      29,      30,      31,\n",
      "            32,      33,      34,      35,      36,      37,      38,      39,\n",
      "            40,      41,      42,      43,      44,      45,      46,      47,\n",
      "            48,      49,      50,      51,      52,      53,      54,      55,\n",
      "            56,      57,      58,      59,      60,      61,      62,      63,\n",
      "            64,      65,      66,      67,      68,      69,      70,      71,\n",
      "            72,      73,      74,      75,      76,      77,      78,      79,\n",
      "            80,      81,      82,      83,      84,      85,      86,      87,\n",
      "            88,      89,      90,      91,      92,      93, 'label'],\n",
      "      dtype='object')\n",
      "        1    2     3    4     5     6    7     8    9      10  ...    84  \\\n",
      "0      2.0  0.0   2.0  0.0   3.0   3.0  2.0  14.0  0.0   22.0  ...   2.0   \n",
      "1      0.0  1.0   0.0  0.0  10.0  10.0  3.0  17.0  1.0   63.0  ...   6.0   \n",
      "2      0.0  0.0   0.0  0.0   0.0   0.0  0.0   7.0  0.0    6.0  ...   0.0   \n",
      "3      7.0  0.0   7.0  0.0   0.0   0.0  1.0  14.0  2.0   21.0  ...   1.0   \n",
      "4      0.0  0.0   0.0  0.0   0.0   0.0  1.0   8.0  0.0    9.0  ...   0.0   \n",
      "5      1.0  0.0   1.0  0.0   1.0   1.0  1.0  27.0  0.0   24.0  ...   3.0   \n",
      "6      2.0  0.0   2.0  0.0   2.0   2.0  1.0  27.0  1.0   53.0  ...   6.0   \n",
      "7      0.0  0.0   0.0  0.0   2.0   2.0  0.0   9.0  0.0   16.0  ...   1.0   \n",
      "8      0.0  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0    1.0  ...   0.0   \n",
      "9      0.0  0.0   0.0  0.0   0.0   0.0  0.0   1.0  0.0    5.0  ...   0.0   \n",
      "10     3.0  1.0   3.0  0.0   4.0   4.0  4.0  42.0  1.0   87.0  ...   3.0   \n",
      "11     3.0  0.0   3.0  0.0   5.0   5.0  1.0   3.0  2.0    8.0  ...   1.0   \n",
      "12     4.0  0.0   4.0  0.0   2.0   2.0  2.0  16.0  0.0   30.0  ...   1.0   \n",
      "13     1.0  0.0   1.0  0.0   1.0   1.0  0.0   6.0  2.0   29.0  ...   1.0   \n",
      "14     0.0  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0    1.0  ...   0.0   \n",
      "15     0.0  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0    1.0  ...   0.0   \n",
      "16     2.0  2.0   2.0  0.0   6.0   6.0  3.0  18.0  2.0   33.0  ...   1.0   \n",
      "17     0.0  0.0   0.0  0.0   1.0   1.0  0.0  10.0  0.0   13.0  ...   1.0   \n",
      "18     3.0  0.0   3.0  0.0   2.0   2.0  0.0  13.0  0.0   19.0  ...   2.0   \n",
      "19    15.0  0.0  15.0  0.0   9.0   9.0  3.0  20.0  1.0   40.0  ...   0.0   \n",
      "20    11.0  0.0  11.0  1.0   2.0   2.0  1.0  20.0  1.0   36.0  ...   0.0   \n",
      "21     1.0  2.0   1.0  0.0   6.0   6.0  1.0  15.0  2.0   17.0  ...   1.0   \n",
      "22     3.0  1.0   3.0  0.0   4.0   4.0  2.0  24.0  1.0   77.0  ...   1.0   \n",
      "23     3.0  0.0   3.0  0.0   4.0   4.0  0.0  30.0  2.0   69.0  ...   3.0   \n",
      "24     0.0  0.0   0.0  0.0   0.0   0.0  0.0   1.0  0.0    7.0  ...   1.0   \n",
      "25     0.0  1.0   0.0  0.0   2.0   2.0  1.0   8.0  0.0   10.0  ...   0.0   \n",
      "26     1.0  0.0   1.0  0.0   1.0   1.0  0.0   8.0  0.0   14.0  ...   2.0   \n",
      "27     1.0  0.0   1.0  0.0   6.0   6.0  1.0   0.0  0.0    9.0  ...   0.0   \n",
      "28     7.0  0.0   7.0  0.0   1.0   1.0  0.0   7.0  0.0    7.0  ...   0.0   \n",
      "29     0.0  0.0   0.0  0.0   0.0   0.0  0.0   1.0  0.0    6.0  ...   2.0   \n",
      "...    ...  ...   ...  ...   ...   ...  ...   ...  ...    ...  ...   ...   \n",
      "1281   1.0  0.0   1.0  0.0   0.0   0.0  0.0   2.0  0.0    6.0  ...   0.0   \n",
      "1282   1.0  0.0   1.0  0.0   0.0   0.0  1.0   2.0  0.0    3.0  ...   0.0   \n",
      "1283   0.0  3.0   0.0  0.0   5.0   5.0  1.0  13.0  0.0   15.0  ...   1.0   \n",
      "1284   0.0  0.0   0.0  0.0   0.0   0.0  0.0   7.0  1.0   11.0  ...   0.0   \n",
      "1285   6.0  0.0   6.0  0.0   9.0   9.0  0.0  85.0  1.0  139.0  ...   8.0   \n",
      "1286   2.0  0.0   2.0  0.0   7.0   7.0  5.0  16.0  0.0   58.0  ...   3.0   \n",
      "1287   0.0  1.0   0.0  0.0   2.0   2.0  0.0  32.0  3.0   52.0  ...   3.0   \n",
      "1288   8.0  0.0   8.0  0.0   0.0   0.0  2.0  26.0  0.0   34.0  ...   1.0   \n",
      "1289   0.0  0.0   0.0  0.0   3.0   3.0  2.0  20.0  0.0   42.0  ...   1.0   \n",
      "1290   1.0  0.0   1.0  0.0   0.0   0.0  1.0   9.0  0.0    8.0  ...   0.0   \n",
      "1291  12.0  0.0  12.0  0.0   7.0   7.0  5.0  23.0  2.0   97.0  ...   3.0   \n",
      "1292  14.0  0.0  14.0  0.0   4.0   4.0  6.0  32.0  0.0   43.0  ...   1.0   \n",
      "1293   0.0  0.0   0.0  0.0   2.0   2.0  0.0   0.0  0.0    6.0  ...   1.0   \n",
      "1294   0.0  0.0   0.0  0.0   0.0   0.0  0.0   1.0  0.0    5.0  ...   0.0   \n",
      "1295   1.0  0.0   1.0  0.0   3.0   3.0  1.0  11.0  0.0   31.0  ...   0.0   \n",
      "1296   1.0  0.0   1.0  0.0   1.0   1.0  1.0  20.0  1.0   30.0  ...   0.0   \n",
      "1297  56.0  0.0  56.0  0.0   9.0   9.0  0.0   0.0  0.0    0.0  ...   0.0   \n",
      "1298   1.0  0.0   1.0  0.0   0.0   0.0  0.0   4.0  0.0    5.0  ...   0.0   \n",
      "1299   1.0  0.0   1.0  0.0   7.0   7.0  0.0  27.0  1.0   70.0  ...   0.0   \n",
      "1300   0.0  2.0   0.0  0.0   3.0   3.0  0.0   7.0  0.0    6.0  ...   1.0   \n",
      "1301   0.0  0.0   0.0  0.0   1.0   1.0  0.0  23.0  0.0   27.0  ...   0.0   \n",
      "1302   0.0  0.0   0.0  0.0   1.0   1.0  0.0  13.0  0.0   17.0  ...   0.0   \n",
      "1303   0.0  0.0   0.0  0.0   0.0   0.0  0.0   7.0  0.0   16.0  ...   1.0   \n",
      "1304   0.0  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0    0.0  ...   0.0   \n",
      "1305   0.0  0.0   0.0  0.0   0.0   0.0  0.0   8.0  0.0   20.0  ...   1.0   \n",
      "1306   3.0  0.0   3.0  0.0   3.0   3.0  2.0   9.0  0.0   28.0  ...   0.0   \n",
      "1307   3.0  2.0   3.0  0.0   6.0   6.0  3.0  76.0  2.0   82.0  ...   5.0   \n",
      "1308   6.0  1.0   6.0  0.0   3.0   3.0  4.0  49.0  1.0   84.0  ...   8.0   \n",
      "1309   4.0  8.0   4.0  0.0   7.0   7.0  5.0  21.0  1.0   49.0  ...   3.0   \n",
      "1310   4.0  0.0   4.0  0.0   0.0   0.0  2.0  22.0  1.0   28.0  ...   0.0   \n",
      "\n",
      "        85   86   87    88   89   90   91   92    93  \n",
      "0      2.0  0.0  1.0   2.0  0.0  1.0  2.0  1.0   2.0  \n",
      "1     10.0  0.0  1.0  17.0  1.0  0.0  1.0  3.0   1.0  \n",
      "2      0.0  1.0  0.0   2.0  0.0  0.0  0.0  1.0   0.0  \n",
      "3      2.0  2.0  1.0   2.0  0.0  0.0  2.0  1.0   2.0  \n",
      "4      0.0  0.0  0.0   3.0  0.0  0.0  0.0  0.0   0.0  \n",
      "5      2.0  0.0  1.0   2.0  0.0  1.0  2.0  0.0   4.0  \n",
      "6     11.0  4.0  0.0  10.0  1.0  1.0  2.0  0.0   0.0  \n",
      "7      2.0  0.0  0.0   3.0  0.0  0.0  1.0  0.0   0.0  \n",
      "8      0.0  1.0  0.0   1.0  0.0  0.0  0.0  0.0   0.0  \n",
      "9      0.0  0.0  0.0   1.0  0.0  0.0  0.0  0.0   1.0  \n",
      "10     5.0  1.0  1.0   6.0  1.0  2.0  4.0  5.0   7.0  \n",
      "11     5.0  0.0  0.0   1.0  0.0  3.0  0.0  1.0   2.0  \n",
      "12     1.0  1.0  1.0   7.0  2.0  1.0  3.0  1.0   1.0  \n",
      "13     1.0  2.0  1.0  10.0  2.0  0.0  1.0  0.0   1.0  \n",
      "14     1.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  \n",
      "15     0.0  1.0  0.0   1.0  0.0  0.0  0.0  0.0   0.0  \n",
      "16     1.0  0.0  0.0   5.0  1.0  3.0  0.0  1.0   2.0  \n",
      "17     2.0  2.0  0.0   0.0  0.0  0.0  3.0  5.0   5.0  \n",
      "18     0.0  2.0  0.0   9.0  0.0  2.0  1.0  1.0   2.0  \n",
      "19     7.0  1.0  1.0   5.0  1.0  0.0  3.0  1.0  10.0  \n",
      "20     1.0  4.0  1.0   6.0  0.0  0.0  5.0  1.0   2.0  \n",
      "21     4.0  1.0  0.0   6.0  0.0  0.0  2.0  0.0   2.0  \n",
      "22     7.0  1.0  1.0  10.0  2.0  1.0  5.0  0.0   2.0  \n",
      "23     7.0  2.0  3.0  12.0  1.0  0.0  8.0  2.0   5.0  \n",
      "24     0.0  0.0  0.0   1.0  0.0  0.0  0.0  0.0   0.0  \n",
      "25     0.0  3.0  1.0   8.0  0.0  3.0  1.0  2.0   2.0  \n",
      "26     3.0  0.0  0.0   2.0  0.0  0.0  2.0  0.0   3.0  \n",
      "27     0.0  0.0  0.0   2.0  1.0  0.0  1.0  0.0   0.0  \n",
      "28     0.0  0.0  0.0   4.0  0.0  0.0  1.0  0.0   1.0  \n",
      "29     1.0  0.0  1.0   4.0  1.0  0.0  0.0  0.0   0.0  \n",
      "...    ...  ...  ...   ...  ...  ...  ...  ...   ...  \n",
      "1281   0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  \n",
      "1282   0.0  0.0  0.0   2.0  0.0  0.0  1.0  0.0   0.0  \n",
      "1283   2.0  0.0  0.0   4.0  0.0  0.0  4.0  3.0   1.0  \n",
      "1284   0.0  1.0  0.0   2.0  1.0  2.0  0.0  1.0   1.0  \n",
      "1285  15.0  4.0  3.0  43.0  6.0  4.0  6.0  4.0  10.0  \n",
      "1286   4.0  1.0  0.0   6.0  1.0  0.0  0.0  0.0   3.0  \n",
      "1287   6.0  1.0  2.0   9.0  1.0  1.0  5.0  6.0   2.0  \n",
      "1288   2.0  1.0  0.0  19.0  1.0  2.0  1.0  2.0   6.0  \n",
      "1289   2.0  1.0  1.0   7.0  2.0  1.0  0.0  6.0   5.0  \n",
      "1290   1.0  0.0  0.0   1.0  0.0  0.0  1.0  0.0   5.0  \n",
      "1291   4.0  1.0  2.0   8.0  0.0  2.0  3.0  5.0   3.0  \n",
      "1292   4.0  2.0  1.0   6.0  0.0  1.0  2.0  0.0   2.0  \n",
      "1293   1.0  0.0  0.0   3.0  1.0  0.0  0.0  0.0   0.0  \n",
      "1294   0.0  1.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  \n",
      "1295   1.0  0.0  2.0   3.0  0.0  0.0  1.0  1.0   4.0  \n",
      "1296   0.0  3.0  0.0   6.0  0.0  1.0  4.0  1.0   2.0  \n",
      "1297   0.0  0.0  0.0  13.0  0.0  0.0  0.0  0.0   0.0  \n",
      "1298   0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   1.0  \n",
      "1299   5.0  0.0  1.0   4.0  1.0  0.0  4.0  3.0   4.0  \n",
      "1300   1.0  0.0  1.0   3.0  0.0  0.0  2.0  0.0   2.0  \n",
      "1301   2.0  0.0  0.0   8.0  0.0  2.0  2.0  1.0   5.0  \n",
      "1302   2.0  0.0  1.0   8.0  0.0  0.0  4.0  1.0   1.0  \n",
      "1303   1.0  0.0  0.0   2.0  0.0  0.0  0.0  1.0   2.0  \n",
      "1304   0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  \n",
      "1305   1.0  0.0  0.0   7.0  0.0  1.0  2.0  1.0   1.0  \n",
      "1306   2.0  0.0  0.0  15.0  3.0  0.0  0.0  1.0   0.0  \n",
      "1307   4.0  2.0  0.0  21.0  0.0  4.0  3.0  5.0   3.0  \n",
      "1308   9.0  2.0  1.0  16.0  2.0  3.0  2.0  4.0   5.0  \n",
      "1309   3.0  2.0  0.0  10.0  2.0  2.0  2.0  1.0   2.0  \n",
      "1310   4.0  0.0  1.0   4.0  3.0  0.0  3.0  6.0  13.0  \n",
      "\n",
      "[1311 rows x 93 columns]\n",
      "[[556 114]\n",
      " [191 450]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.83      0.78       670\n",
      "          1       0.80      0.70      0.75       641\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1311\n",
      "\n",
      "0.767353165523\n",
      "0.834714881133\n"
     ]
    }
   ],
   "source": [
    "print (test_postags_merged.columns)\n",
    "print (test_postags_merged.drop(['label', 'sno'],axis=1))\n",
    "# check the accuracy on the training set\n",
    "lr.score(postags_merged.drop(['label', 'sno'],axis=1), postags_merged['label'])\n",
    "# predict class labels for the test set\n",
    "predicted = lr.predict(test_postags_merged.drop(['label', 'sno'],axis=1))\n",
    "# generate class probabilities\n",
    "probs = lr.predict_proba(test_postags_merged.drop(['label', 'sno'],axis=1))\n",
    "\n",
    "\n",
    "# generate evaluation metrics\n",
    "\n",
    "print (metrics.confusion_matrix(test['label'], predicted))\n",
    "print (metrics.classification_report(test['label'], predicted))\n",
    "print (metrics.accuracy_score(test['label'], predicted))\n",
    "print (metrics.roc_auc_score(test['label'], probs[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfProbs = pd.DataFrame(probs)\n",
    "dfProbs['sno']=test['sno']\n",
    "dfProbs['label']=test['label']\n",
    "dfProbs['text']=test['text'].str.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfProbs.to_csv(\"posttag_probs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_fake_story=\"“What difference, at this point, does it make?” Remember those infamous and wretched words? Of course you do. We all do. Those were the words uttered by Hillary Clinton when she was desperately trying to spin her way through a committee hearing on Benghazi.And as anyone not named Hillary Clinton or anyone not on her corrupt payroll knows, it makes a lot of difference . This because Americans died. They died at the hands of terrorists on Hillary’s watch.New information is now out about some of the individuals who were paid to guard/protect state department employees at the special missions compound where the terrorist attack occurred and it’s not pretty. Turns out Hillary’s state department spent $9.2 million on a contract for guards, many of whom turned out to be the very terrorists who attacked the compound, killing our ambassador and other Americans in the process.Via Fox News .“Many of the local Libyans who attacked the consulate on the night of Sept. 11, 2012, were the actual guards that the State Department under Hillary Clinton hired to protect the Consulate in Benghazi,” Tiegen told Fox News. “The guards were unvetted and were locals with basically no background at all in providing security. Most of them never had held a job in security in the past.“Blue Mountain Libya, at the time of being awarded the contract by our State Department, had no employees so they quickly had to find people to work, regardless of their backgrounds,” he said.One former guard who witnessed the attack, Weeam Mohamed, confirmed in an email sent to the Citizens Commission on Benghazi and obtained by Fox News, that at least four of the guards hired by Blue Mountain took part in the attack after opening doors to allow their confederates in.“In the U.S. Mission, there were four people [who] belonged to the battalion February 17,” Mohamed wrote to the Commission, an independent body formed with Accuracy in Media to investigate the attack and the administration’s handling of it.Again, it makes a lot of difference.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sno    1    2    3    4    5    6    7    8    9 ...    84   85   86  \\\n",
      "0  123.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  8.0  0.0 ...   2.0  1.0  0.0   \n",
      "\n",
      "    87   88   89   90   91   92   93  \n",
      "0  0.0  3.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[1 rows x 94 columns]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.zeros(shape=(1,94))\n",
    "sentListTrain =  sent_tokenize(sample_fake_story)\n",
    "matrix[0,0]=123\n",
    "\n",
    "for sent in sentListTrain:\n",
    "    words= word_tokenize(sent)\n",
    "    recvListTrain=nltk.pos_tag(words)\n",
    "\n",
    "    for tagindex,multiPOSCombo in enumerate(regexAllComboTags):\n",
    "        cp = nltk.RegexpParser(multiPOSCombo)\n",
    "        result = cp.parse(recvListTrain)\n",
    "                \n",
    "        for subtree in result:\n",
    "            if type(subtree) == nltk.tree.Tree:\n",
    "                matrix[0,tagindex + 1]=matrix[0,tagindex + 1] + 1\n",
    "\n",
    "matrix.astype(int)\n",
    "#np.savetxt(\"numpy.out\", matrix, newline=\" \")\n",
    "dfPredict = pd.DataFrame(matrix)\n",
    "dfPredict.rename(columns={0: 'sno'}, inplace=True)\n",
    "\n",
    "print (dfPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Changes: Prediction :  [[ 0.53570821  0.46429179]]\n",
      "Feature No :  1  :  [[ 0.53030462  0.46969538]]\n",
      "Feature No :  2  :  [[ 0.54176573  0.45823427]]\n",
      "Feature No :  3  :  [[ 0.53637162  0.46362838]]\n",
      "Feature No :  4  :  [[ 0.44468354  0.55531646]]\n",
      "Feature No :  5  :  [[ 0.44696536  0.55303464]]\n",
      "Feature No :  6  :  [[ 0.44924941  0.55075059]]\n",
      "Feature No :  7  :  [[ 0.43617831  0.56382169]]\n",
      "Feature No :  8  :  [[ 0.4409141  0.5590859]]\n",
      "Feature No :  9  :  [[ 0.41570653  0.58429347]]\n",
      "Feature No :  10  :  [[ 0.39509475  0.60490525]]\n",
      "Feature No :  11  :  [[ 0.40516972  0.59483028]]\n",
      "Feature No :  12  :  [[ 0.45579354  0.54420646]]\n",
      "Feature No :  13  :  [[ 0.47896305  0.52103695]]\n",
      "Feature No :  14  :  [[ 0.47311185  0.52688815]]\n",
      "Feature No :  15  :  [[ 0.46181499  0.53818501]]\n",
      "Feature No :  16  :  [[ 0.46443321  0.53556679]]\n",
      "Feature No :  17  :  [[ 0.49704964  0.50295036]]\n",
      "Feature No :  18  :  [[ 0.52837561  0.47162439]]\n",
      "Feature No :  19  :  [[ 0.5169004  0.4830996]]\n",
      "Feature No :  20  :  [[ 0.52057546  0.47942454]]\n",
      "Feature No :  21  :  [[ 0.50769341  0.49230659]]\n",
      "Feature No :  22  :  [[ 0.50311884  0.49688116]]\n",
      "Feature No :  23  :  [[ 0.40781681  0.59218319]]\n",
      "Feature No :  24  :  [[ 0.41584468  0.58415532]]\n",
      "Feature No :  25  :  [[ 0.41386509  0.58613491]]\n",
      "Feature No :  26  :  [[ 0.39028642  0.60971358]]\n",
      "Feature No :  27  :  [[ 0.39654176  0.60345824]]\n",
      "Feature No :  28  :  [[ 0.4100738  0.5899262]]\n",
      "Feature No :  29  :  [[ 0.40792626  0.59207374]]\n",
      "Feature No :  30  :  [[ 0.38901996  0.61098004]]\n",
      "Feature No :  31  :  [[ 0.40480562  0.59519438]]\n",
      "Feature No :  32  :  [[ 0.4108783  0.5891217]]\n",
      "Feature No :  33  :  [[ 0.27074711  0.72925289]]\n",
      "Feature No :  34  :  [[ 0.26215984  0.73784016]]\n",
      "Feature No :  35  :  [[ 0.25932762  0.74067238]]\n",
      "Feature No :  36  :  [[ 0.30838257  0.69161743]]\n",
      "Feature No :  37  :  [[ 0.29021092  0.70978908]]\n",
      "Feature No :  38  :  [[ 0.30242929  0.69757071]]\n",
      "Feature No :  39  :  [[ 0.31149851  0.68850149]]\n",
      "Feature No :  40  :  [[ 0.32834211  0.67165789]]\n",
      "Feature No :  41  :  [[ 0.28560505  0.71439495]]\n",
      "Feature No :  42  :  [[ 0.21673946  0.78326054]]\n",
      "Feature No :  43  :  [[ 0.20800378  0.79199622]]\n",
      "Feature No :  44  :  [[ 0.21657201  0.78342799]]\n",
      "Feature No :  45  :  [[ 0.31716804  0.68283196]]\n",
      "Feature No :  46  :  [[ 0.35388541  0.64611459]]\n",
      "Feature No :  47  :  [[ 0.3896339  0.6103661]]\n",
      "Feature No :  48  :  [[ 0.39457642  0.60542358]]\n",
      "Feature No :  49  :  [[ 0.41554632  0.58445368]]\n",
      "Feature No :  50  :  [[ 0.41939308  0.58060692]]\n",
      "Feature No :  51  :  [[ 0.41881672  0.58118328]]\n",
      "Feature No :  52  :  [[ 0.41155644  0.58844356]]\n",
      "Feature No :  53  :  [[ 0.43859931  0.56140069]]\n",
      "Feature No :  54  :  [[ 0.43061385  0.56938615]]\n",
      "Feature No :  55  :  [[ 0.43807169  0.56192831]]\n",
      "Feature No :  56  :  [[ 0.45918334  0.54081666]]\n",
      "Feature No :  57  :  [[ 0.45853718  0.54146282]]\n",
      "Feature No :  58  :  [[ 0.45257541  0.54742459]]\n",
      "Feature No :  59  :  [[ 0.50113196  0.49886804]]\n",
      "Feature No :  60  :  [[ 0.51390204  0.48609796]]\n",
      "Feature No :  61  :  [[ 0.50114017  0.49885983]]\n",
      "Feature No :  62  :  [[ 0.53376527  0.46623473]]\n",
      "Feature No :  63  :  [[ 0.52885149  0.47114851]]\n",
      "Feature No :  64  :  [[ 0.48960886  0.51039114]]\n",
      "Feature No :  65  :  [[ 0.49246042  0.50753958]]\n",
      "Feature No :  66  :  [[ 0.49764382  0.50235618]]\n",
      "Feature No :  67  :  [[ 0.47870812  0.52129188]]\n",
      "Feature No :  68  :  [[ 0.46323489  0.53676511]]\n",
      "Feature No :  69  :  [[ 0.43340373  0.56659627]]\n",
      "Feature No :  70  :  [[ 0.42808062  0.57191938]]\n",
      "Feature No :  71  :  [[ 0.43034326  0.56965674]]\n",
      "Feature No :  72  :  [[ 0.45628195  0.54371805]]\n",
      "Feature No :  73  :  [[ 0.46038899  0.53961101]]\n",
      "Feature No :  74  :  [[ 0.63794481  0.36205519]]\n",
      "Feature No :  75  :  [[ 0.62595043  0.37404957]]\n",
      "Feature No :  76  :  [[ 0.67718771  0.32281229]]\n",
      "Feature No :  77  :  [[ 0.69493844  0.30506156]]\n",
      "Feature No :  78  :  [[ 0.5806758  0.4193242]]\n",
      "Feature No :  79  :  [[ 0.62588496  0.37411504]]\n",
      "Feature No :  80  :  [[ 0.53130769  0.46869231]]\n",
      "Feature No :  81  :  [[ 0.52489704  0.47510296]]\n",
      "Feature No :  82  :  [[ 0.51265746  0.48734254]]\n",
      "Feature No :  83  :  [[ 0.48390214  0.51609786]]\n",
      "Feature No :  84  :  [[ 0.46685551  0.53314449]]\n",
      "Feature No :  85  :  [[ 0.44542839  0.55457161]]\n",
      "Feature No :  86  :  [[ 0.4550884  0.5449116]]\n",
      "Feature No :  87  :  [[ 0.45664226  0.54335774]]\n",
      "Feature No :  88  :  [[ 0.49670514  0.50329486]]\n",
      "Feature No :  89  :  [[ 0.49720331  0.50279669]]\n",
      "Feature No :  90  :  [[ 0.51464423  0.48535577]]\n",
      "Feature No :  91  :  [[ 0.50529575  0.49470425]]\n",
      "Feature No :  92  :  [[ 0.51320973  0.48679027]]\n"
     ]
    }
   ],
   "source": [
    "testDF = pd.DataFrame([])\n",
    "print (\"Before Changes: Prediction : \",lr.predict_proba(dfPredict.drop(['sno'],axis=1)))\n",
    "for i in range(1,93):\n",
    "    testDf = dfPredict\n",
    "    testDf[i] = testDf[i] + 1\n",
    "    print (\"Feature No : \",i,\" : \",lr.predict_proba(testDf.drop(['sno'],axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
